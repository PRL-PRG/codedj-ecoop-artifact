# Introduction

This is the artifact for the paper *CodeDJ: Reproducible Queries over 
Large-Scale Software Repositories* submitted to ECOOP 2021. The artifact 
consists of the following parts:

- **Getting started** A walkthrough through the setting up the system from 
  scratch and executing queries;
- **Experiment** A re-creation of the experiment from the paper.

This is an R markdown document. It's like a markdown document, but has 
executable snippets of code that do the work (in various languages, including
bash, Python, R). It can be viewed using `rstudio`---we recommend doing this.
It can also be executed with R using the following script:

```bash
TODO
```

This will compile the document into HTML while executing all the code snippets.

## Paper details

**CodeDJ: Reproducible Queries over Large-Scale Software Repositories**

Petr Maj. CTU Prague.  
Konrad Siek. CTU Prague.  
Alexander Kovalenko. CTU Prague.   
Jan Vitek. CTU Prague and Northeastern.  

**Abstract** Analyzing massive code bases is a staple of modern software
engineering research – a welcome side-effect of the advent of large-scale
software repositories such as GitHub. Selecting projects to analyze is a
labor-intensive process that can lead to biased results if the chosen projects
are not representative. One issue is that the interface exposed by software
repositories only allows formulation of the most basic of queries. Code DJ is an
infrastructure for querying repositories composed of a persistent datastore,
constantly updated with data acquired GitHub, and an in-memory database with a
Rust query interface. Code DJ supports reproducibility, historical queries are
answered deterministically using historical states of the datastore; thus
researchers can reproduce published results. To illustrate the benefits of Code
DJ , we identify biases in the data of a published study and, by repeating the
analysis with new data, we demonstrate that its conclusions were sensitive to
the choice of projects.

## Part 1: Getting started

This part describes setting up the CodeDJ system from scratch, downloading a 
dataset, and running a query.

Prerequisites:

- [git](https://git-scm.com/), 
- [Rust and cargo](https://www.rust-lang.org/tools/install), 
- TODO libs.

In order to download projects from GitHub Parasite requires the user have a 
GitHub account and a personal access token. You can generate a token for your 
GitHub account by following the instructions 
[here](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token). 
The token does not need any scopes or permissions.

### Setup

(You can run all the code in this section by executing `scripts/setup.sh`.)

Downloading and building Parasite (the GitHub dataset downloader):

```{bash}
git clone https://github.com/PRL-PRG/codedj-parasite.git parasite
cd parasite
cargo build --release
cd ..
```

Downloading and building Djanco (the query engine):

```{bash}
git clone https://github.com/PRL-PRG/djanco.git 
cd djanco 
cargo build --release
cd ..
```

### Downloading a toy dataset

(You can run all the code in this section by executing `scripts/download.sh`. 
You will have to exit parasite interactive mode manually after the download 
finishes.)

We explain how to download a small 10-project dataset using Parasite. We also 
provide a pre-downloaded repository in `pregenerated/toy-dataset` so this step
can be skipped.

To create a basic dataset, first create a directory that will contain the 
downloaded. Be aware that datasets tend to be large. 

```{bash}
mkdir -p toy-dataset/
mkdir -p toy-dataset/repo_clones
```

Next, specify the list of repositories to include in the dataset in a CSV file.
 The toy dataset will contain the following 10 repositories (4 Python, 4 
 JavaScript, and 2 TypeScript repositories):

```
repository
https://github.com/nodejs/node.git
https://github.com/pixijs/pixi.js.git
https://github.com/angular/angular.git
https://github.com/apache/airflow.git
https://github.com/facebook/react.git
https://github.com/vuejs/vue.git
https://github.com/xonsh/xonsh.git
https://github.com/meteor/meteor.git
https://github.com/3b1b/manim.git
https://github.com/s0md3v/photon.git
```

This file is located at `toy-dataset-repositories.csv`.

Then, feed the list of repositories to Parasite:

```{bash}
parasite/target/release/parasite --datastore toy-dataset add toy-dataset-repositories.csv
```

Next, create a CSV file containing one of more 
[GitHub personal access tokens](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token). 
No special scopes or permissions are needed on the token. These are used to 
download data using the GitHub REST API and are essential for the downloader
to work.

```
token
fa56454....
```

We cannot provide a file with these for presentation purposes. We assume the
reader prepares their own token file in the current directory at 
`ghtokens.csv`.

The next step is to enter interactive console in Parasite. Provide a path to
the GitHub token file via the ght flag. You can also specify the number of
threads that the downloader will use with the n flag (here we use 8).

```{bash}
parasite/target/release/parasite --datastore toy-dataset -ght ghtokens.csv -n 8 --interactive
```

![](img/interactive.png)

In interactive console: execute the loadall command to load substore
information into memory.

```
loadall
```

![](img/loadall.png)

Then, also in *interactive* console: execute `updateall` to start the
downloader. This will cause Parasite to download, process and store information
about each added repository using 8 threads.

```
updateall
```

Wait until the download completes (about 15 minutes for the example dataset).
Exit the downloader (`^C`). The example dataset is ready for querying. 

### Setting up a query

There are two ways in which a query can be written and executed. We prepared a
system that will generate a cargo crate and then generate a run script for all
the queries in the crate. This is the easiest way to use Djanco. Alternatively,
one can create a cargo crate from scratch. We only present the former here.

A pre-generated instance of the crate generated by this process is included in
`pregenerated/my-query-crate`.

First, install the `generate` command for the `cargo` system.

```{bash}
cargo install cargo-generate
```


Then, install the `djanco` command for the `cargo` system.

```{bash}
cargo install --git https://github.com/PRL-PRG/cargo-djanco
```

Generate a cargo crate:

```{bash}
cargo generate --git https://github.com/PRL-PRG/djanco-query-template --name my-query-crate
cd my-query-crate
```

This creates a fully-configured cargo create at location`my-query-crate`
with the following directory structure:

```
my-query-crate/
├── Cargo.toml
├── README.md
└── src
    └── lib.rs
```

Inside the crate, there is a `lib.rs` file with an example query that selects
the top starred project in each language using all available subsets of the
repository.

```rust
use std::path::Path;

use djanco::*;
use djanco::data::*;
use djanco::log::*;
use djanco::csv::*;

use djanco_ext::*;

#[djanco(May, 2021, subsets(All))]
pub fn my_query(database: &Database, _log: &Log, output: &Path) -> Result<(), std::io::Error>  {
    database.projects()
        .group_by(project::Language)
        .sort_by(project::Stars)
        .sample(Top(1))
        .into_csv_in_dir(output, "top_1_project_by_stars_in_each_language.csv")
}
```

Generate a run script for the queries:

```{bash}
cargo djanco
```

This generates a rust program at location `src/bin/djanco.rs` that initializes
the dataset and runs all functions in the crate that are tagged as 
`#[djanco(...)]`.

### Running the query

Build and execute the query using the toy dataset:

```{bash}
cargo run --release --bin djanco -- --dataset-path ../toy-dataset --cache-path cache --output-path output
```

After the query is executed, the results of the query will be available at 
`output/top_1_project_by_stars_in_each_language.csv` [truncated]:

```
language,project_id,substore,url, [...] ,stars, ...
JavaScript,5,JavaScript,https://github.com/vuejs/vue.git, [...] ,181894, [...]
TypeScript,2,TypeScript,https://github.com/angular/angular.git, [...] ,72384, [...]
Python,8,Python,https://github.com/3b1b/manim.git, [...] ,32791, [...]
```

We attach a pregenerated instance of the output of this query at 
`pregenerated/top_1_project_by_stars_in_each_language.csv`. Note that the
values of attributes will have changed over time and the downloader will
acquire the most recent values---preserving historical data requires
updating a dataset over time.

```{bash}
cd ..
```

## Part 2: Experiment

Thie section prepares the data and plots the graphs for Section 4 *A Case
Study: Of Bugs and Languages* in ther paper. This requires downloading the
data, performing the queries, and processing the results.

### Download the dataset

The dataset used in the analysis in the paper is constructed by downloading the
repositories listed in `paper-dataset-repositories.csv` containing 229,933 
GitHub URLs. We provide a pre-downloaded dataset in 
`pregrenerated/paper-dataset`.

We do not recommend downloading it yourself since it will take a significant
amount of time and disk space. Nevertheless, you can. You require a
`ghtokens.csv` file containing at least one valid github personal access
token.

```
token
fa56454....
```

Create a directory for the dataset:
```bash
mkdir -p dataset/
mkdir -p dataset/repo_clones
```

Then, add the repositories to the dataset:

```bash
parasite/target/release/parasite --datastore paper-dataset add paper-dataset-repositories.csv
```

Then, turn on interactive mode and start the download by executing 
`loadall` followed by `updateall`.

```bash
parasite/target/release/parasite --datastore paper-dataset -ght ghtokens.csv -n 8 --interactive
```

Wait until completed.

### Perform project selection

There are 7 project selection queries in the paper:

* Stars: Pick projects with most stars. Rationale: starred projects are 
  popular and thus likely to be well written and maintained. 
* Touched Files: compute #files changed by commits, pick projects that
  changed the most files. Rationale: indicative of projects where commits 
  represent larger units of work.
* Experienced Author: experienced developers are those on GitHub for at 
  least two years; pick a sample of projects with at least one experienced 
  contributor. Rationale: less likely to be throw-away projects.
* 50% Experienced: projects with two or more developers, half of which 
  experienced. Rationale: focus on larger teams.
* Message Size: Compute size in bytes of commit messages; pick projects 
  with the largest size. Rationale: empty or trivial commit messages 
  indicate uninteresting projects.
* Number of Commits: Compute the number of commits; pick projects with the 
  most commits. Rationale: larger projects are more mature.
* Issues: Pick projects with the most issues. Rationale: issues indicate 
  a more structured development process.

The queries used for project selection are provided in directory 
`paper-queries`. This is a Rust crate. This crate was created by the same
procedure as presented in section 1 of this document. The source code of all
the queries can be found in `paper-queries/src/lib.rs`. The queries are
executed by running `paper-queries/src/bin/djanco.rs`.

To execute the queries on the pregenereated dataset run:

```{bash}
cd paper-queries
cargo run --bin djanco --release -- --dataset-path ../pregenerated/paper-dataset --cache-path ../cache/pregenerated/ --output-path ../output
cd ...
```

To execute the queries on freshly-downloaded dataset run:

```bash
cd paper-queries
cargo run --bin djanco --release -- --dataset-path ../paper-dataset --cache-path ../cache/fresh/ --output-path ../query-results
cd ...
```

The query results are generated into the directory `query-results`. They consist of
the following files:

```
TODO
```

### Run analysis and generate graphs

This section processes the results from the previous sections and generates
figures. Additional data is attached to the project list from the dataset. 
Then, data is processd and analyzed using R. The final part of the analysis and
graph generation is done in Python. 

All the code snipepts are embedded in this Rmd file and are one-click
executable from RStudio.

#### Preprocessing

Convert the results of the queries into a format that can be used for further analysis. First, attach additional data to match the original FSE paper.

```{bash}
parasite/target/release/ecoop-artifact --datastore dataset export query-results/stars.list                       query-results/stars.csv
parasite/target/release/ecoop-artifact --datastore dataset export query-results/commits.list                     query-results/commits.csv
parasite/target/release/ecoop-artifact --datastore dataset export query-results/experienced_author.list          query-results/experienced_author.csv
parasite/target/release/ecoop-artifact --datastore dataset export query-results/experienced_authors_ratio.list   query-results/experienced_authors_ratio.csv
parasite/target/release/ecoop-artifact --datastore dataset export query-results/mean_changes_in_commits.list     query-results/mean_changes_in_commits.csv
parasite/target/release/ecoop-artifact --datastore dataset export query-results/median_commit_message_sizes.list query-results/median_commit_message_sizes.csv
parasite/target/release/ecoop-artifact --datastore dataset export query-results/all_issues.list                  query-results/all_issues.csv
parasite/target/release/ecoop-artifact --datastore dataset export --all                                          query-results/full_dataset.csv
```

Then, prepare another set of results containing summaries of key characteristics.

```{bash}
parasite/target/release/ecoop-artifact --datastore dataset compact query-results/stars.csv                       query-results/stars_p.csv                       query-results/stars_pl.csv      
parasite/target/release/ecoop-artifact --datastore dataset compact query-results/commits.csv                     query-results/commits_p.csv                     query-results/commits_pl.csv 
parasite/target/release/ecoop-artifact --datastore dataset compact query-results/experienced_authors.csv         query-results/experienced_authors_p.csv         query-results/experienced_authors_pl.csv
parasite/target/release/ecoop-artifact --datastore dataset compact query-results/experienced_authors_ratio.csv   query-results/experienced_authors_ratio_p.csv   query-results/experienced_authors_ratio_pl.csv
parasite/target/release/ecoop-artifact --datastore dataset compact query-results/mean_changes_in_commits.csv     query-results/mean_changes_in_commits_p.csv       query-results/mean_changes_in_commits_pl.csv
parasite/target/release/ecoop-artifact --datastore dataset compact query-results/median_commit_message_sizes.csv query-results/median_commit_message_sizes_p.csv query-results/median_commit_message_sizes_pl.csv
parasite/target/release/ecoop-artifact --datastore dataset compact query-results/all_issues.csv                  query-results/all_issues_p.csv                  query-results/all_issues_pl.csv 
parasite/target/release/ecoop-artifact --datastore dataset compact query-results/full_dataset.csv                query-results/full_dataset_p.csv                query-results/full_dataset_pl.csv
```

#### Preparation 

Load auxiliary functions and initialize log:

```{R, include=FALSE}
knitr::knit_engines$set(python = reticulate::eng_python)
source("scripts/implementation.R")
initializeLog()
```

Load the results of the queries:

```{r, include=FALSE}
INPUT_DIR <- "query-results/" # output of queries, input of this script
MODEL_DIR <- "models/"
GRAPHS_DIR <- "graphs/"

dir.create(MODEL_DIR)
dir.create(GRAPHS_DIR)

STARS <- paste0(INPUT_DIR, "stars.csv")
EXPERIENCED_AUTHORS <- paste0(INPUT_DIR, "experienced_authors.csv")
EXPERIENCED_AUTHORS_RATIO <- paste0(INPUT_DIR, "experienced_authors_ratio.csv")
MEAN_CHANGES_IN_COMMITS <- paste0(INPUT_DIR, "mean_changes_in_commits.csv")
MEDIAN_COMMIT_MESSAGE_SIZES <- paste0(INPUT_DIR, "median_commit_message_sizes.csv")
COMMITS <- paste0(INPUT_DIR, "commits.csv")
ALL_ISSUES <- paste0(INPUT_DIR, "all_issues.csv")
FULL_DATASET <- paste0(INPUT_DIR, "full-dataset.csv")

datasets_raw <- list(
  stars = load_dataset(STARS),
  changes = load_dataset(MEAN_CHANGES_IN_COMMITS),
  expa = load_dataset(EXPERIENCED_AUTHORS),
  expr = load_dataset(EXPERIENCED_AUTHORS_RATIO),
  messages = load_dataset(MEDIAN_COMMIT_MESSAGE_SIZES),
  commits = load_dataset(COMMITS),
  issues = load_dataset(ALL_ISSUES)
)
```

Now we fix the datasets, calculating the missing commit age and filtering small projects and small project-language pairs:

```{r, include=FALSE}
fix_dataset = function(dataset, dataset_name) {
    before = nrow(dataset)
    # remove projects which have fewer than 28 commits 
    small_projects = dataset %>% group_by(project) %>% summarize(commits = n_distinct(sha)) %>% filter(commits >= 28)
    dataset = dataset %>% filter(project %in% small_projects$project)
    # calculate commit age
    dataset = dataset %>% group_by(project) %>% mutate(min_date = min(commit_date)) %>% mutate(commit_age = max(1, as.integer((commit_date - min_date)/(24 * 3600)))) %>% dplyr::select(-c(min_date))
    # and remove small groups, i.e. those that contain less than 20 commits (this was done in the original paper
    dataset = dataset %>% group_by(project, language) %>% mutate(n = n()) %>% filter(n > 20) %>% dplyr::select(-c(n))
    # we are done    
    after = nrow(dataset)
    LOG(paste0("selection",dataset_name,"before"), before)
    LOGPctAndRaw(paste0("selection",dataset_name,"after"), after, before)
    # and return the updated dataset
    dataset
}
datasets = Map(fix_dataset, datasets_raw, names(datasets_raw))
```

Calculate the models:

```{r, include=FALSE}
fix_dataset = function(dataset, dataset_name) {
    before = nrow(dataset)
    # remove projects which have fewer than 28 commits 
    small_projects = dataset %>% group_by(project) %>% summarize(commits = n_distinct(sha)) %>% filter(commits >= 28)
    dataset = dataset %>% filter(project %in% small_projects$project)
    # calculate commit age
    dataset = dataset %>% group_by(project) %>% mutate(min_date = min(commit_date)) %>% mutate(commit_age = max(1, as.integer((commit_date - min_date)/(24 * 3600)))) %>% dplyr::select(-c(min_date))
    # and remove small groups, i.e. those that contain less than 20 commits (this was done in the original paper
    dataset = dataset %>% group_by(project, language) %>% mutate(n = n()) %>% filter(n > 20) %>% dplyr::select(-c(n))
    # we are done    
    after = nrow(dataset)
    LOG(paste0("selection",dataset_name,"before"), before)
    LOGPctAndRaw(paste0("selection",dataset_name,"after"), after, before)
    # and return the updated dataset
    dataset
}
datasets = Map(fix_dataset, datasets_raw, names(datasets_raw))
```

#### Domain knowledge

Creates a figure that, for each language, calculates the value of the 
coefficients (higher means more bugs); the queries returned 50 projects in 
each of the 17 target languages: Coefficients that are not statistically 
significant are shown in faded colors. If the input set did not matter for the
model, one could expect the different queries to give roughly the same 
coefficients with the same significance. This is not the case. If we focus on how many languages have
statistically significant coefficients: The touched files query is highly predictive, 14 of the
languages are significant, but the coefficients are frequently opposite from those of other
queries. Specifically, C is associated with slightly fewer bugs, so are C#, CoffeeScript, Java,
JavaScript, Objective-C, Perl, PHP, Python, Ruby and TypeScript. On the other hand C++,
Erlang, Go and Haskell are associated with more defects. This is striking as it goes against
expectations. The stars query is the least informative. It only gives 7 statistically significant
coefficients with remarkably low values.

```{r, include=FALSE}
summarizeByLanguage = function(what) {
    what %>% 
        group_by(project, language) %>%
        dplyr::summarize(
            commits = n_distinct(sha),
            tins = sum(insertion),
            max_commit_age = max(commit_age),
            bcommits = sum(isbug),
            #domain = unique(domain),
            devs = n_distinct(devs)
        )
}

logTransform = function(what, log1 = log, log2 = log) {
    data.frame(
        language = what$language, 
        ldevs = log1(what$devs),
        lcommits=log1(what$commits),
        ltins=log2(what$tins),
        lmax_commit_age=log1(what$max_commit_age),
        lbcommits=log2(what$bcommits + 0.5*(what$bcommits==0)),
        bcommits=what$bcommits,
        #combined=factor(what$combined),
        #domain=factor(what$domain),
        #domain_r = relevel(what$domain, rev(levels(what$domain))[1]),
        language_r = relevel(what$language, rev(levels(what$language))[1]),
        commits = what$commits
        #combined_r = relevel(what$combined, rev(levels(what$combined))[1])
    )
}

# Weighted contrasts as described and used by the authors of the original paper
contr.Weights <- function(fac)
{
    fDist=summary(fac)
    fSum=contr.sum(levels(fac))		
    fSum[nrow(fSum),] = -fDist[1:ncol(fSum)]/fDist[length(fDist)]
    fSum
}

# Takes the glm model and the releveled second model for the last observation and combines them together returning a single data frame
combineModels = function(model, model_r, var, pValAdjust = "none") {
    controlVariables = 4
    s = summary(model)$coefficients
    s_r = summary(model_r)$coefficients
    rownames = getModelRowNames(model, var)
    coef = round(c(s[,1], s_r[controlVariables + 2, 1]), 2)
    se = round(c(s[,2], s_r[controlVariables + 2, 2]), 2)
    pVal = c(s[,4], s_r[controlVariables + 2, 4])
    if (pValAdjust == "bonferroni" || pValAdjust == "fdr")
        pVal[(controlVariables + 2):length(pVal)] = p.adjust(pVal[(controlVariables + 2):length(pVal)], pValAdjust)
    #pVal = round(pVal, 3)
    names(coef) = rownames
    data.frame(
        coef, 
        se,
        pVal
    )
} 

getModelRowNames = function(model, var) {
    controlVariables = 4
    rownames = c(dimnames(summary(model)$coefficients)[[1]][1:(1 + controlVariables)], names(summary(var)))
    names(rownames) = rownames
    rownames[["(Intercept)"]] = "Intercept"
    rownames[["lmax_commit_age"]] = "log age"
    rownames[["ltins"]] = "log size"
    rownames[["ldevs"]] = "log devs"
    rownames[["lcommits"]] = "log commits"
    rownames
}

calculateModel = function(dataset, dataset_name) {
    cat(paste0(nrow(dataset)," ", dataset_name, "\n"))
    X = summarizeByLanguage(dataset)
    X$language = droplevels(X$language)
    #Y = logTransform(X, log10, log)
    Y = logTransform(X, log, log)
    # fit the negative binomial regression
    weights = contr.Weights(Y$language)
    nbfit = glm.nb(bcommits~lmax_commit_age+ltins+ldevs+lcommits+language, contrasts = list(language = contr.Weights(Y$language)), data=Y)
    nbfit_r = glm.nb(bcommits~lmax_commit_age+ltins+ldevs+lcommits+language_r, contrasts = list(language_r = contr.Weights(Y$language_r)), data=Y)
    # combine them into single result table
    result = combineModels(nbfit, nbfit_r, Y$language)
    result$pVal = round(result$pVal, digits = 3)
    result$name = as.factor(rownames(result))
    result$dataset = dataset_name
    result$signifficant = result$pVal <= 0.05 # orly? 
    full_names = data.frame(name = as.factor(c("Intercept", "log age", "log size", "log devs", "log commits", levels(dataset$language))))
    result = left_join(full_names, result, by = c("name"))
    rownames(result) = full_names$name
    result
}

output_model = function(model, name) {
  #model$coef[model$coef > 0.5] = 0.5
  #model$coef[model$coef < -0.5] = -0.5
  write.csv(model %>% dplyr::select(coef, pVal), paste0(MODEL_DIR, name, ".csv"))
}

models <- Map(calculateModel, datasets, names(datasets))
Map(output_model, models, names(models))
```

Use these models to create a graph:

```{r, include=FALSE}
# Install any uninstalled Python packages for reticulate
py_install("pandas")
py_install("numpy")
py_install("matplotlib")
py_install("seaborn")
```

```{python fig.width=24, fig.height:8}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import time
from datetime import datetime
from matplotlib.colors import LogNorm
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.patches as mpatches

sns.set(style='white', rc={'figure.figsize':(14,8)})
plt.rcParams.update({'font.size': 42})

MICRO_SIZE = 16
SMALL_SIZE = 20
MEDIUM_SIZE = 26
BIGGER_SIZE = 32

plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title
plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels
plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('legend', fontsize=MICRO_SIZE)    # legend fontsize
plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title

from os import listdir
from os.path import isfile, join

files = [f for f in listdir(r.MODEL_DIR) if isfile(join(r.MODEL_DIR, f))]
print(files)

#names
names = ['Stars',
      'Touched Files',
      '50% Experienced',
      'Experienced Author',
      'Message Size',
      'Number of Commits',
      'Issues']

frames = []

for f in files:
    temp = pd.read_csv(r.MODEL_DIR + f)[5:]
    print("temp")
    print(temp)
    frames.append(temp)

df_smart = pd.concat(frames, ignore_index=True, sort=False)
df_smart.rename(columns={'Unnamed: 0': 'Language:'}, inplace=True)
df_smart['split'] = sum([[f]*17 for f in files], []) #change to names

df_smart_sig = df_smart.copy()
df_smart_insig = df_smart.copy()

df_smart_sig.loc[(df_smart_sig['pVal'] <= 0.05), 'coef'] = 0.0
df_smart_insig.loc[(df_smart_insig['pVal'] > 0.05), 'coef'] = 0.0

fig, ax = plt.subplots()
ax2 = ax
ax3 = ax

sns.barplot(x='Language:',
               y='coef',
               data=df_smart_sig,
               hue='split',
               #palette='Greens',
               edgecolor='k',
               #color=('blue'),
               ax = ax)

sns.barplot(x='Language:',
               y='coef',
               data=df_smart_insig,
               hue='split',
               #palette='Greens',
               edgecolor='k',
               alpha = 1.0,
               ax=ax2,
               #legend=False,
               color='white')

      
sns.barplot(x='Language:',
               y='coef',
               data=df_smart_insig,
               hue='split',
               #palette='Greens',
               edgecolor='k',
               alpha = 0.2,
               ax=ax3,
               #legend=False
               #color=('blue'),
               )         

plt.xlabel(None)
plt.ylabel('Coefficient')
ax.set_xticklabels(ax.get_xticklabels(), rotation=45)
plt.xlim((-0.42, 16.43))

print(handles)
print(labels)

handles, labels = ax.get_legend_handles_labels()
ax.legend(handles[0:7], names[0:7], frameon=False, loc = "best") 

for i in range(9):
    ax.axvspan(-0.4 + 2*i, 0.60 + 2*i, facecolor='gray', alpha=0.1)

offset = 0.0632

plt.tight_layout()
plt.savefig(f"{r.GRAPHS_DIR}/fig6_domain_knowledge.pdf") # Domain knowledge
plt.show()
```

The plot is created in `graphs/fig6_domain_knowledge.pdf`.

### Project Size and Age Distributions

To help, the paper provides distributions of
various measures in the data. This figure visualizes the distribution of project sizes (left) and
project age (right) for the entire dataset and for the all the queries queries.

Looking at these distributions makes it clear that the projects returned by various queries
are quite different in character. The experienced author and the number of commits are
remarkably similar and return projects that meet our expectations. The issues distribution
is similar, which should raise red flags given that it frequently disagrees. The stars query
returns many smaller projects. Finally, message sizes and touched files show distributions
opposite to those expected. They favor degenerate young projects with few commits that are
either verbose, or disproportionately large (touching over 100k files). This is reflected also in
the input sizes, ranging from 8M rows for the experienced author query to mere 79K rows of
the touched files query. It is likely that these queries are “wrong” in the sense they do not
return the population of interest. The Figure also suggest that stars is not a good choice.

```{python fig.width=14, fig.height:9}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.colors import LogNorm
from os import listdir
from os.path import isfile, join

stuff = [f for f in listdir(r.INPUT_DIR) if isfile(join(r.INPUT_DIR, f))]

df_names = ['full_dataset_p.csv',
            'stars_p.csv',
            'experienced_authors_ratio_p.csv',
            'experienced_authors_p.csv',
            'commits_p.csv',
            'median_commit_message_sizes_p.csv',
            'all_issues_p.csv',
            'mean_changes_in_commits_p.csv',
          ]

frames = []

for i,f in enumerate(df_names):
    temp = pd.read_csv(r.INPUT_DIR + f)
    temp['name'] = [names[i]] * len(temp.index)
    frames.append(temp)

names = ['Full dataset',
         'Stars',
         '50% Experienced',     
         'Experienced Author',
         'Number of Commits',
         'Message Size',      
         'Issues',
         'Touched Files'
        ]

frames = []

for i,f in enumerate(df_names):
    temp = pd.read_csv(r.INPUT_DIR + f)
    temp['name'] = [names[i]] * len(temp.index)
    frames.append(temp)

df_full = pd.concat(frames, ignore_index=True, sort=False)
df_full['plotValue'] = np.log10(df_full.commits)
df_full['name_value'] = ['Commits']*len(df_full.index)

df_full_age = pd.concat(frames, ignore_index=True, sort=False)
df_full_age['plotValue'] = np.log10(df_full.age/3600/24+1)
df_full_age['name_value'] = ['Age [days]']*len(df_full.index)

df_full = pd.concat([df_full, df_full_age], ignore_index=True, sort=False)

sns.set(style='white', rc={'figure.figsize':(14,8)}) #24,8 for big one
plt.rcParams.update({'font.size': 42})

MICRO_SIZE = 16
SMALL_SIZE = 20
MEDIUM_SIZE = 26
BIGGER_SIZE = 32

plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title
plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels
plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('legend', fontsize=MICRO_SIZE)    # legend fontsize
plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title

flatui = ["#82E0AA", "#FF5733", "#2ecc71", "#e74c3c", "#9b59b6", "#3498db", "#95a5a6", "#34495e"]
sns.set_palette(flatui)

fig, ax = plt.subplots()

ax2 = ax.twinx()

sns.violinplot(x='name',
               y='plotValue',
               data=df_full,
               width=0.9,
               hue='name_value',
               split=True,
               cut = 0,
               #palette='Greens',
               ax=ax
              )

ax.set(ylabel = "$\mathregular{log_{10}}$ (Commits)", xlabel = None)
ax2.set(ylabel = "$\mathregular{log_{10}}$ (Age [days])", xlabel = None)
ax.set_xticklabels(ax.get_xticklabels(), rotation=45)

ax.legend(frameon=True, loc = "upper right") 

ax.set_ylim([-0.1, 5.3])
ax2.set_ylim([-0.1, 5.3])

plt.tight_layout()
plt.savefig(f"{r.GRAPHS_DIR}/fig7_project_size_and_age_distribution.pdf")
plt.show()
```














#### Project selection summary

```{r, include=FALSE}
SUMMARY_PROJECTS_STARS <- paste0(INPUT_DIR, "stars_p.csv")
SUMMARY_PROJECTS_MEAN_CHANGES_IN_COMMITS <- paste0(INPUT_DIR, "mean_changes_in_commits_p.csv")
SUMMARY_PROJECTS_EXPERIENCED_AUTHORS <- paste0(INPUT_DIR, "experienced_authors_p.csv")
SUMMARY_PROJECTS_EXPERIENCED_AUTHORS_RATIO <- paste0(INPUT_DIR, "experienced_authors_ratio_p.csv")
SUMMARY_PROJECTS_MEAN_CHANGES_IN_COMMITS <- paste0(INPUT_DIR, "mean_changes_in_commits_p.csv")
SUMMARY_PROJECTS_MEDIAN_COMMIT_MESSAGE_SIZES <- paste0(INPUT_DIR, "median_commit_message_sizes_p.csv")
SUMMARY_PROJECTS_COMMITS <- paste0(INPUT_DIR, "commits_p.csv")
SUMMARY_PROJECTS_ALL_ISSUES <- paste0(INPUT_DIR, "all_issues_p.csv")
SUMMARY_PROJECTS_FULL_DATASET <- paste0(INPUT_DIR, "full_dataset_p.csv")

SUMMARY_LANGUAGES_STARS <- paste0(INPUT_DIR, "stars_pl.csv")
SUMMARY_LANGUAGES_MEAN_CHANGES_IN_COMMITS <- paste0(INPUT_DIR, "mean_changes_in_commits_pl.csv")
SUMMARY_LANGUAGES_EXPERIENCED_AUTHORS <- paste0(INPUT_DIR, "experienced_authors_pl.csv")
SUMMARY_LANGUAGES_EXPERIENCED_AUTHORS_RATIO <- paste0(INPUT_DIR, "experienced_authors_ratio_pl.csv")
SUMMARY_LANGUAGES_MEAN_CHANGES_IN_COMMITS <- paste0(INPUT_DIR, "mean_changes_in_commits_pl.csv")
SUMMARY_LANGUAGES_MEDIAN_COMMIT_MESSAGE_SIZES <- paste0(INPUT_DIR, "median_commit_message_sizes_pl.csv")
SUMMARY_LANGUAGES_COMMITS <- paste0(INPUT_DIR, "commits_pl.csv")
SUMMARY_LANGUAGES_ALL_ISSUES <- paste0(INPUT_DIR, "all_issues_pl.csv")
SUMMARY_LANGUAGES_FULL_DATASET <- paste0(INPUT_DIR, "full_dataset_pl.csv")

summarize_dataset = function (dataset, name) {
  dataset %>% 
    group_by(language) %>% 
    dplyr::summarize(
        commits_all = n(),
        commits_distinct = n_distinct(sha),
        projects = n_distinct(project),
        mean_files = mean(files),
        median_files = median(files),
        bugs = sum(isbug)) %>%
    mutate(selection = name)
}

load_summary = function(path) {
  result = read_delim(path, delim=',', escape_double=FALSE, escape_backslash=TRUE, quote="\"")
  result$language = as.factor(tolower(result$language))
  result$major = result$pctCommits >= 50
  result$pctCommits = result$pctCommits / 100
  invisible(result)
}

load_projects_summary = function(path) {
  result = read_delim(path, delim=',', escape_double=FALSE, escape_backslash=TRUE, quote="\"")
  invisible(result)
}
```

```{r, include=FALSE}
sums = Reduce(rbind, Map(summarize_dataset, datasets, names(datasets)))

summary_p = list(
  stars = load_projects_summary(SUMMARY_PROJECTS_STARS),
  changes = load_projects_summary(SUMMARY_PROJECTS_MEAN_CHANGES_IN_COMMITS),
  expa = load_projects_summary(SUMMARY_PROJECTS_EXPERIENCED_AUTHORS),
  expr = load_projects_summary(SUMMARY_PROJECTS_EXPERIENCED_AUTHORS_RATIO),
  messages = load_projects_summary(SUMMARY_PROJECTS_MEDIAN_COMMIT_MESSAGE_SIZES),
  commits = load_projects_summary(SUMMARY_PROJECTS_COMMITS),
  issues = load_projects_summary(SUMMARY_PROJECTS_ALL_ISSUES),
  full = load_projects_summary(SUMMARY_PROJECTS_FULL_DATASET)
)

summary_p

big_table_p = Reduce(rbind, Map(function(dataset, name) { dataset %>% mutate(name = name) }, summary_p, names(summary_p)))
big_table_p$name = factor(big_table_p$name, c("full","changes","commits","expa","expr","issues","messages","stars"))
# just to check number of lines
nrow(summary_p$full)
```

Remove minor languages:

```r
remove_minor_languages = function(dataset, dataset_name) {
  before = nrow(dataset)
  dataset$language = tolower(as.character(dataset$language))
  result = inner_join(dataset, major_languages, by = c("project", "language"))
  after = nrow(result)
  LOG(paste0("major",dataset_name,"before"), before)
  LOGPctAndRaw(paste0("major",dataset_name,"after"), after, before)
  result$language = as.factor(result$language)
  result
}
major_languages = read_delim("/data/with_results/dataset-descriptions/dataset-projects.csv", 
                             delim=',', escape_double=FALSE, escape_backslash=TRUE, quote="\"") %>% 
  dplyr::select(project = id, language) %>% 
  dplyr::mutate(language=tolower(as.character(language)))
major_datasets = Map(remove_minor_languages, datasets, names(datasets))
```

Calculate coefficients for major languages:

```{r}
major_models = Map(calculateModel, major_datasets, names(major_datasets))
Map(function(model, name) write.csv(model %>% dplyr::select(coef, pVal), paste0(MODEL_DIR, name, "_major.csv")), 
    major_models, names(major_models))
```

