~~~
This is an R markdown document. It's like a markdown document, but has 
executable snippets of code that do the work (in various languages, including
bash, Python, R). It can be viewed using RStudio---we recommend doing this.

    rstudio 1-getting-started.Rmd

It can also be executed (in whole) with R using the following bash snippet:*
 
    Rscript -e "rmarkdown::render('1-getting-started.Rmd')"

This will compile the document into HTML while executing all the code 
snippets in the document.

This is a tutorial on how to use codedj from scratch, building your own dataset
and running own queries and does not have to be executed in order to simply
reproduce the paper claims.

~~~


# Part 1: Getting started

This part describes setting up the CodeDJ system from scratch, downloading a
dataset, and running a query. It is a tutorial aimed at someone who is trying
out getting their own dataset, and writing their own queries. This part
consists of the following sections:

- **Setup**---how to download and build the Parasite and Djanco,
- **Skipping downloading**---how to use a pre-downloaded dataset without
  downloading anything,
- **Downloading a dataset**---how to download a dataset from scratch,
- **Setting up a query**---how to create a Rust crate with Djanco queries,
- **Running a query**---how to execute a Djanco query.

### Setup

Djanco and Parasite are already installed in the artifact image but you can 
download and build them by using the following commands:

```{bash}
rm -rf parasite
git clone https://github.com/PRL-PRG/codedj-parasite.git parasite
cd parasite
git checkout ecoop-artifact
cargo build --release
cd ..
```

```{bash}
rm -rf djanco
git clone https://github.com/PRL-PRG/djanco.git djanco
cd djanco 
git checkout ecoop-artifact
cargo build --release
cd ..
```

### Downloading a toy dataset

We explain how to download a small 10-project dataset using Parasite. 

In order to download projects from GitHub, Parasite requires the user have a
GitHub account and a personal access token. You can generate a token for your
GitHub account by following the instructions
[here](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token).
The token does not need any scopes or permissions.

To create a basic dataset, first create a directory that will contain the
downloaded. Be aware that datasets tend to be large. 

```{bash}
mkdir -p datasets/toy-dataset/
mkdir -p datasets/toy-dataset/repo_clones
```

Next, specify the list of repositories to include in the dataset in a CSV file.
The toy dataset will contain the following 10 repositories (4 Python, 4
JavaScript, and 2 TypeScript repositories):

```
repository
https://github.com/nodejs/node.git
https://github.com/pixijs/pixi.js.git
https://github.com/angular/angular.git
https://github.com/apache/airflow.git
https://github.com/facebook/react.git
https://github.com/vuejs/vue.git
https://github.com/xonsh/xonsh.git
https://github.com/meteor/meteor.git
https://github.com/3b1b/manim.git
https://github.com/s0md3v/photon.git
```

This file is located at `datasets/toy-dataset-repositories.csv`.

Then, feed the list of repositories to Parasite:

```{bash}
parasite/target/release/parasite --datastore datasets/toy-dataset add datasets/toy-dataset-repositories.csv
```

Next, create a CSV file containing one of more [GitHub personal access
tokens](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token).
No special scopes or permissions are needed on the token. These are used to
download data using the GitHub REST API and are essential for the downloader to
work.

```
token
fa56454....
```

We cannot provide a file with these for presentation purposes. We assume the
reader prepares their own token file in the current directory at
`ghtokens.csv`.

The next step is to enter interactive console in Parasite. Provide a path to
the GitHub token file via the ght flag. You can also specify the number of
threads that the downloader will use with the n flag (here we use 8).

```bash
parasite/target/release/parasite --datastore datasets/toy-dataset -ght ghtokens.csv -n 8 --interactive
```

(This executes the command in a new terminal, so it's possible to run in from
inside RStudio)

```{bash}
gnome-terminal -e "parasite/target/release/parasite --datastore datasets/toy-dataset -ght ghtokens.csv -n 8 --interactive"
```

![](img/interactive.png)

In interactive console: execute the `loadall` command to load `substore`
information into memory.

```
loadall
```

![](img/loadall.png)

Then, also in *interactive* console, execute `updateall` to start the
downloader. This will cause Parasite to download, process and store information
about each added repository using 8 threads.

```
updateall
```

Wait until the download completes (about 15 minutes for the example dataset).
Exit the downloader (`^C`). The example dataset is ready for querying. 

### Setting up a query

There are two ways in which a query can be written and executed. We prepared a
system that will generate a cargo crate and then generate a run script for all
the queries in the crate. This is the easiest way to use Djanco. Alternatively,
one can create a cargo crate from scratch. We only present the former here.

A pre-generated instance of the crate generated by this process is included in
`queries/my-query-crate`. 

Optionally you can re-generate this cargo crate. To do this, the `generate`
command must be installed in `cargo`:

```{bash}
cargo install cargo-generate
```

Then, generate a new cargo crate from a template (remove the pre-existing crate):

```{bash}
cd queries 
rm -rf my-query-crate
cargo generate --git https://github.com/PRL-PRG/djanco-query-template --name my-query-crate
cd ..
```

This creates a fully-configured cargo create at location`my-query-crate`
with the following directory structure:

```
my-query-crate/
├── Cargo.toml          # Confiuguration file for the crate: metadata, dependencies
├── README.md           # Automatically generated README file: installation and usage
└── src                 # Source directory
    └── lib.rs          # Toplevel source file: contains queries
```

Inside the crate, there is a `lib.rs` file with an example query that selects
the top starred project in each language using all available subsets of the
repository.

```rust
use std::path::Path;

use djanco::*;
use djanco::data::*;
use djanco::log::*;
use djanco::csv::*;

use djanco_ext::*;

#[djanco(May, 2021, subsets(All))]
pub fn my_query(database: &Database, _log: &Log, output: &Path) -> Result<(), std::io::Error>  {
    database.projects()
        .group_by(project::Language)
        .sort_by(project::Stars)
        .sample(Top(1))
        .into_csv_in_dir(output, "top_1_project_by_stars_in_each_language.csv")
}
```

The next step is to generate a run script for the queries. This requires that
the `djanco` command be installed in `cargo`:

```
cargo install --git https://github.com/PRL-PRG/cargo-djanco
```

Then, generate a run script:

```{bash}
cd queries/my-query-crate
cargo djanco
cd ../..
```

This generates a Rust program at location `src/bin/djanco.rs` that initializes
the dataset and runs all functions in the crate that are tagged as
`#[djanco(...)]`. 

### Running the query

Prior to running the query, build the crate:

```{bash}
cd queries/my-query-crate
cargo build --release
cd ..
```

Execute the query using the toy dataset:

```{bash}
cd queries/my-query-crate
cargo run --release --bin djanco -- --dataset-path ../../datasets/toy-dataset --cache-path cache --output-path ../../query-results/getting-started/
cd ..
```

After the query is executed, the results of the query will be available at
`query-results/getting-started/top_1_project_by_stars_in_each_language.csv`
[truncated]:

```
language,project_id,substore,url, [...] ,stars, ...
JavaScript,5,JavaScript,https://github.com/vuejs/vue.git, [...] ,181894, [...]
TypeScript,2,TypeScript,https://github.com/angular/angular.git, [...] ,72384, [...]
Python,8,Python,https://github.com/3b1b/manim.git, [...] ,32791, [...]
```

We attach a pre-generated instance of the output of this query at
`query-results/getting-started/pregenerated-top_1_project_by_stars_in_each_language.csv`.
Note that the values of attributes will have changed over time and the
downloader will acquire the most recent values---preserving historical data
requires updating a dataset over time.
