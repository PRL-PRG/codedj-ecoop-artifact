~~~
NOTE

This is an R markdown document. It's like a markdown document, but has 
executable snippets of code that do the work (in various languages, including
bash, Python, R). It can be viewed using Rstudio---we recommend doing this.

    rstudio 2.3-case-study-data-analysis-and-graphs

It can also be executed (in whole) with R using the following bash snippet:*
 
    Rscript -e "rmarkdown::render('2.3-case-study-data-analysis-and-graphs.Rmd')"

This will compile the document into HTML while executing all the code 
snippets in the document.
~~~


# Part 2.3. Case study: data analysis and graphs

Thie section prepares plots the graphs for Section 4 *A Case
Study: Of Bugs and Languages* in the paper. The document contains a section 
for each of the four plots in the paper and a section for preparing the data.

- Preparation
- Random subsets (Fig. 4)
- TypeScript over time (Fig. 5)
- Domain knowledge (Fig. 6)
- Domain knowledge (Fig. 7)

The graphs can be generated in any order, but all of them depend on the preparation 
section having been executed. Scripts in this section do not download anything 
and run in reasonable times. 

All the graphs are generated in place in the Rmd file as well as PDFs in `/graphs`.

## Preparation 

Load auxiliary functions and initialize log:

```{R}
knitr::knit_engines$set(python = reticulate::eng_python)
source("scripts/implementation.R")
initializeLog()
```

Load the results of the queries:

```{r}
INPUT_DIR <- "query-results/paper/" # output of queries, input of this script
MODEL_DIR <- "models/"
GRAPHS_DIR <- "graphs/"

dir.create(MODEL_DIR)
dir.create(GRAPHS_DIR)

STARS <- paste0(INPUT_DIR, "stars.csv")
EXPERIENCED_AUTHORS <- paste0(INPUT_DIR, "experienced_authors.csv")
EXPERIENCED_AUTHORS_RATIO <- paste0(INPUT_DIR, "experienced_authors_ratio.csv")
MEAN_CHANGES_IN_COMMITS <- paste0(INPUT_DIR, "mean_changes_in_commits.csv")
MEDIAN_COMMIT_MESSAGE_SIZES <- paste0(INPUT_DIR, "median_commit_message_sizes.csv")
COMMITS <- paste0(INPUT_DIR, "commits.csv")
ALL_ISSUES <- paste0(INPUT_DIR, "all_issues.csv")
FULL_DATASET <- paste0(INPUT_DIR, "full_dataset.csv")

FSE_REFERENCE <- "query-results/fse/reference.csv"

datasets_raw <- list(
  stars = load_dataset(STARS),
  changes = load_dataset(MEAN_CHANGES_IN_COMMITS),
  expa = load_dataset(EXPERIENCED_AUTHORS),
  expr = load_dataset(EXPERIENCED_AUTHORS_RATIO),
  messages = load_dataset(MEDIAN_COMMIT_MESSAGE_SIZES),
  commits = load_dataset(COMMITS),
  issues = load_dataset(ALL_ISSUES)
)
```

Now we fix the datasets, calculating the missing commit age and filtering small projects and small project-language pairs:

```{r}
fix_dataset = function(dataset, dataset_name) {
    before = nrow(dataset)
    # remove projects which have fewer than 28 commits 
    small_projects = dataset %>% group_by(project) %>% summarize(commits = n_distinct(sha)) %>% filter(commits >= 28)
    dataset = dataset %>% filter(project %in% small_projects$project)
    # calculate commit age
    dataset = dataset %>% group_by(project) %>% mutate(min_date = min(commit_date)) %>% mutate(commit_age = max(1, as.integer((commit_date - min_date)/(24 * 3600)))) %>% dplyr::select(-c(min_date))
    # and remove small groups, i.e. those that contain less than 20 commits (this was done in the original paper
    dataset = dataset %>% group_by(project, language) %>% mutate(n = n()) %>% filter(n > 20) %>% dplyr::select(-c(n))
    # we are done    
    after = nrow(dataset)
    LOG(paste0("selection",dataset_name,"before"), before)
    LOGPctAndRaw(paste0("selection",dataset_name,"after"), after, before)
    # and return the updated dataset
    dataset
}
datasets = Map(fix_dataset, datasets_raw, names(datasets_raw))
```

Calculate the coefficients for various languages.

```{r}
summarizeByLanguage = function(what) {
    what %>% 
        group_by(project, language) %>%
        dplyr::summarize(
            commits = n_distinct(sha),
            tins = sum(insertion),
            max_commit_age = max(commit_age),
            bcommits = sum(isbug),
            #domain = unique(domain),
            devs = n_distinct(devs)
        )
}

logTransform = function(what, log1 = log, log2 = log) {
    data.frame(
        language = what$language, 
        ldevs = log1(what$devs),
        lcommits=log1(what$commits),
        ltins=log2(what$tins),
        lmax_commit_age=log1(what$max_commit_age),
        lbcommits=log2(what$bcommits + 0.5*(what$bcommits==0)),
        bcommits=what$bcommits,
        #combined=factor(what$combined),
        #domain=factor(what$domain),
        #domain_r = relevel(what$domain, rev(levels(what$domain))[1]),
        language_r = relevel(what$language, rev(levels(what$language))[1]),
        commits = what$commits
        #combined_r = relevel(what$combined, rev(levels(what$combined))[1])
    )
}

# Weighted contrasts as described and used by the authors of the original paper
contr.Weights <- function(fac)
{
    fDist=summary(fac)
    fSum=contr.sum(levels(fac))		
    fSum[nrow(fSum),] = -fDist[1:ncol(fSum)]/fDist[length(fDist)]
    fSum
}

# Takes the glm model and the releveled second model for the last observation and combines them together returning a single data frame
combineModels = function(model, model_r, var, pValAdjust = "none") {
    controlVariables = 4
    s = summary(model)$coefficients
    s_r = summary(model_r)$coefficients
    rownames = getModelRowNames(model, var)
    coef = round(c(s[,1], s_r[controlVariables + 2, 1]), 2)
    se = round(c(s[,2], s_r[controlVariables + 2, 2]), 2)
    pVal = c(s[,4], s_r[controlVariables + 2, 4])
    if (pValAdjust == "bonferroni" || pValAdjust == "fdr")
        pVal[(controlVariables + 2):length(pVal)] = p.adjust(pVal[(controlVariables + 2):length(pVal)], pValAdjust)
    #pVal = round(pVal, 3)
    names(coef) = rownames
    data.frame(
        coef, 
        se,
        pVal
    )
} 

getModelRowNames = function(model, var) {
    controlVariables = 4
    rownames = c(dimnames(summary(model)$coefficients)[[1]][1:(1 + controlVariables)], names(summary(var)))
    names(rownames) = rownames
    rownames[["(Intercept)"]] = "Intercept"
    rownames[["lmax_commit_age"]] = "log age"
    rownames[["ltins"]] = "log size"
    rownames[["ldevs"]] = "log devs"
    rownames[["lcommits"]] = "log commits"
    rownames
}

calculateModel = function(dataset, dataset_name) {
    cat(paste0(nrow(dataset)," ", dataset_name, "\n"))
    X = summarizeByLanguage(dataset)
    X$language = droplevels(X$language)
    #Y = logTransform(X, log10, log)
    Y = logTransform(X, log, log)
    # fit the negative binomial regression
    weights = contr.Weights(Y$language)
    nbfit = glm.nb(bcommits~lmax_commit_age+ltins+ldevs+lcommits+language, contrasts = list(language = contr.Weights(Y$language)), data=Y)
    nbfit_r = glm.nb(bcommits~lmax_commit_age+ltins+ldevs+lcommits+language_r, contrasts = list(language_r = contr.Weights(Y$language_r)), data=Y)
    # combine them into single result table
    result = combineModels(nbfit, nbfit_r, Y$language)
    result$pVal = round(result$pVal, digits = 3)
    result$name = as.factor(rownames(result))
    result$dataset = dataset_name
    result$signifficant = result$pVal <= 0.05 # orly? 
    full_names = data.frame(name = as.factor(c("Intercept", "log age", "log size", "log devs", "log commits", levels(dataset$language))))
    result = left_join(full_names, result, by = c("name"))
    rownames(result) = full_names$name
    result
}

models = Map(calculateModel, datasets, names(datasets))
Map(function(model, name) {write.csv(model %>% dplyr::select(coef, pVal), paste0(MODEL_DIR,"/",name,".csv"))} , models, names(models))
```

#### Random subsets

Our first experiment explores the distribution of possible analysis outcomes. For this, we
repeatedly pick a random subset of 50 projects of each of the 17 languages and fit them with
NBR. Fig 4 shows the distribution of the coefficients obtained by 1000 such random selections
compared to the results obtained in [FSE2014] (shown as a tick to the right of the distribution).
Positive values indicate a higher association of the language with defects.
The spread of each distribution is a measure of the sensitivity of the analysis to its inputs.
Intuitively, consider the distribution of coefficients for Objective-C, it is roughly centered
around 0. This means, that a random input is about equally likely to say that the language
has a positive association with defects as a negative one. One could argue that picking close
to the median of the distribution could give a representative answer. As we can see the FSE
paper often picks subsets that are outliers; see the cases of CoffeeScript, Go, Perl, Scala and
most strikingly TypeScript.

Please note that the number of samples used for generating the graph should not be larger than 
the number of samples generated during post-processing the dataset (10000 by default). Also note 
that the more samples are used, the slower the processing becomes.

```{python, fig.width=14, fig.height=8}
SAMPLES = 10000

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.colors import LogNorm
from os import listdir
from os.path import isfile, join

sns.set(style='white', rc={'figure.figsize':(14,8)}) #24,8 for big one
plt.rcParams.update({'font.size': 42})

MICRO_SIZE = 16
SMALL_SIZE = 20
MEDIUM_SIZE = 26
BIGGER_SIZE = 32

plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title
plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels
plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('legend', fontsize=MICRO_SIZE)    # legend fontsize
plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title

ref_FSE = [0.15, 0.23, 0.03, -0.29, -0.07, 0, -0.08, -0.23, -0.01, 0.06, 0.18, -0.15, 0.15, 0.1, -0.15, -0.28, -0.43]

frames = []

for i in range(1, SAMPLES + 1):
    temp = pd.read_csv(f'{r.MODEL_DIR}/ts_by_year/2020/{str(i)}2020_new.csv')
    frames.append(temp[5:])

ref = pd.read_csv(r.FSE_REFERENCE)[5:]
ref['coef'] = ref_FSE

df_dist = pd.concat(frames, ignore_index=True, sort=False)
df_dist = pd.concat([df_dist, ref], ignore_index=True, sort=False)
df_dist['split'] = ['ours']*(len(df_dist) - 17) + ['their']*17
df_dist.rename(columns={'Unnamed: 0': 'Language:'}, inplace=True)

plt.figure(figsize=(14,8))
f = sns.violinplot(x='Language:',
               y='coef',
               data=df_dist,
               width=0.9,
               palette='Greens',
               hue='split',
               split=True
              )
plt.axhline(0, ls='--', color='black')
plt.legend().set_visible(False)
plt.title('')
plt.xlabel(None)
plt.ylabel('Coefficient')
f.set_xticklabels(f.get_xticklabels(), rotation=90)
plt.ylim((-0.7, 0.65))

plt.axhline(0.54, 0.77, 0.82, ls='-', color='black')
plt.text(13.8, 0.52, 'FSE 2014')

plt.tight_layout()
plt.savefig(f"{r.GRAPHS_DIR}/fig4_random_subsets.pdf")
plt.show()
```

#### TypeScript over time

As we have more data than was available in 2013, we can use Code DJ to select inputs at
various times. Here we create eight datasets, each containing data up to one of the years
between 2013 and 2020. For simplicity, we only plot the distribution of coefficients for
TypeScript. The original paper’s coefficient was −.43 (shown as a red line). The graph
clearly shows that the value was an outlier. The association with bugs shifted over time,
increasing to a relatively stable position from 2016.

While it is reasonable to expect variations from year to year, TypeScript experienced a
rather large shift over a short period. The language was released in 2012, so there were few
projects on GitHub in 2013. Furthermore, a number of human language translation files
were misidentified as TypeScript; these files did not have bugs, biasing the result. The rising
popularity of TypeScript quickly caused real code to crowd out the translation files, and the
association with bugs settled to around 0.2.

Please note that the number of samples used for generating the graph should not be larger than 
the number of samples generated during post-processing the dataset (10000 by default). Also note 
that the more samples are used, the slower the processing becomes.

```{python, fig.width=14, fig.height=8}
SAMPLES = 1000

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.colors import LogNorm
from os import listdir
from os.path import isfile, join

sns.set(style='white', rc={'figure.figsize':(14,8)}) #24,8 for big one
plt.rcParams.update({'font.size': 42})

MICRO_SIZE = 16
SMALL_SIZE = 20
MEDIUM_SIZE = 26
BIGGER_SIZE = 32

plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title
plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels
plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('legend', fontsize=MICRO_SIZE)    # legend fontsize
plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title

years = range(2013, 2021)
frames_concat = []
for year in years:
    frames = []
    print(year)
    temp_concat = []
    for i in range(1, SAMPLES + 1):
        #file =   f'{r.MODEL_DIR}/by_year/{str(year)}/{str(i)}{str(year)}_new.csv'
        file = f'{r.MODEL_DIR}/ts_by_year/{str(year)}/{str(i)}{str(year)}_new.csv'
        print(file)
        temp = pd.read_csv(file)
        temp['year'] = [int(year)]*len(temp)
        frames.append(temp[21:])
        temp_concat = pd.concat(frames, ignore_index=True, sort=False)
        
    frames_concat.append(temp_concat)
    
df_concat = pd.concat(frames_concat, ignore_index=True, sort=False)
df_concat.rename(columns={'Unnamed: 0': 'Language:'}, inplace=True)

df_ref = pd.DataFrame({'year': [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020],
                       'coef': [-0.43]*8,
                       'Language:': ['Typescript']*8,
                       'check' : ['issue']*8
                      })
                      
df_concat = pd.concat([df_concat, df_ref], ignore_index=True, sort=False)                  
df_concat['check'] = ['all']*(len(df_concat) - 1) + ['issues']

#colors = ["faded green", "pale red", "windows blue", "amber", "greyish", "dusty purple"]
#sns.palplot(sns.xkcd_palette(colors))

plt.figure(figsize=(24,12))
f = sns.violinplot(x='year',
               y='coef',
               data=df_concat,
               width=0.9,
               split=True,
               hue='check',
               palette='Greens'
              )
plt.axhline(0, ls='--', color = 'black')
plt.legend().set_visible(False)
#plt.title('.TS over time')
plt.xlabel(None)
plt.ylabel('Coefficient')
f.set_xticklabels(f.get_xticklabels(), rotation=45)
plt.text(2.4, -0.6, 'FSE 2014')
plt.ylim((-0.72, 0.6))
plt.axhline(-0.43, 0.0, 1.0, ls='-', color='red')

#plt.tight_layout()
plt.savefig(f"{r.GRAPHS_DIR}/fig5_typescript_over_time.pdf")
plt.show()
```

#### Domain knowledge

Creates a figure that, for each language, calculates the value of the 
coefficients (higher means more bugs); the queries returned 50 projects in 
each of the 17 target languages: Coefficients that are not statistically 
significant are shown in faded colors. If the input set did not matter for the
model, one could expect the different queries to give roughly the same 
coefficients with the same significance. This is not the case. If we focus on how many languages have
statistically significant coefficients: The touched files query is highly predictive, 14 of the
languages are significant, but the coefficients are frequently opposite from those of other
queries. Specifically, C is associated with slightly fewer bugs, so are C#, CoffeeScript, Java,
JavaScript, Objective-C, Perl, PHP, Python, Ruby and TypeScript. On the other hand C++,
Erlang, Go and Haskell are associated with more defects. This is striking as it goes against
expectations. The stars query is the least informative. It only gives 7 statistically significant
coefficients with remarkably low values.

```{r}
summarizeByLanguage = function(what) {
    what %>% 
        group_by(project, language) %>%
        dplyr::summarize(
            commits = n_distinct(sha),
            tins = sum(insertion),
            max_commit_age = max(commit_age),
            bcommits = sum(isbug),
            #domain = unique(domain),
            devs = n_distinct(devs)
        )
}

logTransform = function(what, log1 = log, log2 = log) {
    data.frame(
        language = what$language, 
        ldevs = log1(what$devs),
        lcommits=log1(what$commits),
        ltins=log2(what$tins),
        lmax_commit_age=log1(what$max_commit_age),
        lbcommits=log2(what$bcommits + 0.5*(what$bcommits==0)),
        bcommits=what$bcommits,
        #combined=factor(what$combined),
        #domain=factor(what$domain),
        #domain_r = relevel(what$domain, rev(levels(what$domain))[1]),
        language_r = relevel(what$language, rev(levels(what$language))[1]),
        commits = what$commits
        #combined_r = relevel(what$combined, rev(levels(what$combined))[1])
    )
}

# Weighted contrasts as described and used by the authors of the original paper
contr.Weights <- function(fac)
{
    fDist=summary(fac)
    fSum=contr.sum(levels(fac))		
    fSum[nrow(fSum),] = -fDist[1:ncol(fSum)]/fDist[length(fDist)]
    fSum
}

# Takes the glm model and the releveled second model for the last observation and combines them together returning a single data frame
combineModels = function(model, model_r, var, pValAdjust = "none") {
    controlVariables = 4
    s = summary(model)$coefficients
    s_r = summary(model_r)$coefficients
    rownames = getModelRowNames(model, var)
    coef = round(c(s[,1], s_r[controlVariables + 2, 1]), 2)
    se = round(c(s[,2], s_r[controlVariables + 2, 2]), 2)
    pVal = c(s[,4], s_r[controlVariables + 2, 4])
    if (pValAdjust == "bonferroni" || pValAdjust == "fdr")
        pVal[(controlVariables + 2):length(pVal)] = p.adjust(pVal[(controlVariables + 2):length(pVal)], pValAdjust)
    #pVal = round(pVal, 3)
    names(coef) = rownames
    data.frame(
        coef, 
        se,
        pVal
    )
} 

getModelRowNames = function(model, var) {
    controlVariables = 4
    rownames = c(dimnames(summary(model)$coefficients)[[1]][1:(1 + controlVariables)], names(summary(var)))
    names(rownames) = rownames
    rownames[["(Intercept)"]] = "Intercept"
    rownames[["lmax_commit_age"]] = "log age"
    rownames[["ltins"]] = "log size"
    rownames[["ldevs"]] = "log devs"
    rownames[["lcommits"]] = "log commits"
    rownames
}

calculateModel = function(dataset, dataset_name) {
    cat(paste0(nrow(dataset)," ", dataset_name, "\n"))
    X = summarizeByLanguage(dataset)
    X$language = droplevels(X$language)
    #Y = logTransform(X, log10, log)
    Y = logTransform(X, log, log)
    # fit the negative binomial regression
    weights = contr.Weights(Y$language)
    nbfit = glm.nb(bcommits~lmax_commit_age+ltins+ldevs+lcommits+language, contrasts = list(language = contr.Weights(Y$language)), data=Y)
    nbfit_r = glm.nb(bcommits~lmax_commit_age+ltins+ldevs+lcommits+language_r, contrasts = list(language_r = contr.Weights(Y$language_r)), data=Y)
    # combine them into single result table
    result = combineModels(nbfit, nbfit_r, Y$language)
    result$pVal = round(result$pVal, digits = 3)
    result$name = as.factor(rownames(result))
    result$dataset = dataset_name
    result$signifficant = result$pVal <= 0.05 # orly? 
    full_names = data.frame(name = as.factor(c("Intercept", "log age", "log size", "log devs", "log commits", levels(dataset$language))))
    result = left_join(full_names, result, by = c("name"))
    rownames(result) = full_names$name
    result
}

output_model = function(model, name) {
  #model$coef[model$coef > 0.5] = 0.5
  #model$coef[model$coef < -0.5] = -0.5
  write.csv(model %>% dplyr::select(coef, pVal), paste0(MODEL_DIR, name, ".csv"))
}

models <- Map(calculateModel, datasets, names(datasets))
Map(output_model, models, names(models))
```

Use these models to create a graph:

```{r}
# Install any uninstalled Python packages for reticulate
py_install("pandas")
py_install("numpy")
py_install("matplotlib")
py_install("seaborn")
```

```{python fig.width=24, fig.height:8}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import time
from datetime import datetime
from matplotlib.colors import LogNorm
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.patches as mpatches

sns.set(style='white', rc={'figure.figsize':(14,8)})
plt.rcParams.update({'font.size': 42})

MICRO_SIZE = 16
SMALL_SIZE = 20
MEDIUM_SIZE = 26
BIGGER_SIZE = 32

plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title
plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels
plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('legend', fontsize=MICRO_SIZE)    # legend fontsize
plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title

from os import listdir
from os.path import isfile, join

files = [f for f in listdir(r.MODEL_DIR) if isfile(join(r.MODEL_DIR, f))]
print(files)

#names
names = ['Stars',
      'Touched Files',
      '50% Experienced',
      'Experienced Author',
      'Message Size',
      'Number of Commits',
      'Issues']

frames = []

for f in files:
    temp = pd.read_csv(r.MODEL_DIR + f)[5:]
    print("temp")
    print(temp)
    frames.append(temp)

df_smart = pd.concat(frames, ignore_index=True, sort=False)
df_smart.rename(columns={'Unnamed: 0': 'Language:'}, inplace=True)
df_smart['split'] = sum([[f]*17 for f in files], []) #change to names

df_smart_sig = df_smart.copy()
df_smart_insig = df_smart.copy()

df_smart_sig.loc[(df_smart_sig['pVal'] <= 0.05), 'coef'] = 0.0
df_smart_insig.loc[(df_smart_insig['pVal'] > 0.05), 'coef'] = 0.0

fig, ax = plt.subplots()
ax2 = ax
ax3 = ax

sns.barplot(x='Language:',
               y='coef',
               data=df_smart_sig,
               hue='split',
               #palette='Greens',
               edgecolor='k',
               #color=('blue'),
               ax = ax)

sns.barplot(x='Language:',
               y='coef',
               data=df_smart_insig,
               hue='split',
               #palette='Greens',
               edgecolor='k',
               alpha = 1.0,
               ax=ax2,
               #legend=False,
               color='white')

      
sns.barplot(x='Language:',
               y='coef',
               data=df_smart_insig,
               hue='split',
               #palette='Greens',
               edgecolor='k',
               alpha = 0.2,
               ax=ax3,
               #legend=False
               #color=('blue'),
               )         

plt.xlabel(None)
plt.ylabel('Coefficient')
ax.set_xticklabels(ax.get_xticklabels(), rotation=45)
plt.xlim((-0.42, 16.43))

print(handles)
print(labels)

handles, labels = ax.get_legend_handles_labels()
ax.legend(handles[0:7], names[0:7], frameon=False, loc = "best") 

for i in range(9):
    ax.axvspan(-0.4 + 2*i, 0.60 + 2*i, facecolor='gray', alpha=0.1)

offset = 0.0632

plt.tight_layout()
plt.savefig(f"{r.GRAPHS_DIR}/fig6_domain_knowledge.pdf") # Domain knowledge
plt.show()
```

The plot is created in `graphs/fig6_domain_knowledge.pdf`.

### Project Size and Age Distributions

To help, the paper provides distributions of
various measures in the data. This figure visualizes the distribution of project sizes (left) and
project age (right) for the entire dataset and for the all the queries queries.

Looking at these distributions makes it clear that the projects returned by various queries
are quite different in character. The experienced author and the number of commits are
remarkably similar and return projects that meet our expectations. The issues distribution
is similar, which should raise red flags given that it frequently disagrees. The stars query
returns many smaller projects. Finally, message sizes and touched files show distributions
opposite to those expected. They favor degenerate young projects with few commits that are
either verbose, or disproportionately large (touching over 100k files). This is reflected also in
the input sizes, ranging from 8M rows for the experienced author query to mere 79K rows of
the touched files query. It is likely that these queries are “wrong” in the sense they do not
return the population of interest. The Figure also suggest that stars is not a good choice.

```{python fig.width=14, fig.height:9}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.colors import LogNorm
from os import listdir
from os.path import isfile, join

stuff = [f for f in listdir(r.INPUT_DIR) if isfile(join(r.INPUT_DIR, f))]

df_names = ['full_dataset_p.csv',
            'stars_p.csv',
            'experienced_authors_ratio_p.csv',
            'experienced_authors_p.csv',
            'commits_p.csv',
            'median_commit_message_sizes_p.csv',
            'all_issues_p.csv',
            'mean_changes_in_commits_p.csv',
          ]

frames = []

for i,f in enumerate(df_names):
    temp = pd.read_csv(r.INPUT_DIR + f)
    temp['name'] = [names[i]] * len(temp.index)
    frames.append(temp)

names = ['Full dataset',
         'Stars',
         '50% Experienced',     
         'Experienced Author',
         'Number of Commits',
         'Message Size',      
         'Issues',
         'Touched Files'
        ]

frames = []

for i,f in enumerate(df_names):
    temp = pd.read_csv(r.INPUT_DIR + f)
    temp['name'] = [names[i]] * len(temp.index)
    frames.append(temp)

df_full = pd.concat(frames, ignore_index=True, sort=False)
df_full['plotValue'] = np.log10(df_full.commits)
df_full['name_value'] = ['Commits']*len(df_full.index)

df_full_age = pd.concat(frames, ignore_index=True, sort=False)
df_full_age['plotValue'] = np.log10(df_full.age/3600/24+1)
df_full_age['name_value'] = ['Age [days]']*len(df_full.index)

df_full = pd.concat([df_full, df_full_age], ignore_index=True, sort=False)

sns.set(style='white', rc={'figure.figsize':(14,8)}) #24,8 for big one
plt.rcParams.update({'font.size': 42})

MICRO_SIZE = 16
SMALL_SIZE = 20
MEDIUM_SIZE = 26
BIGGER_SIZE = 32

plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title
plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels
plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('legend', fontsize=MICRO_SIZE)    # legend fontsize
plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title

flatui = ["#82E0AA", "#FF5733", "#2ecc71", "#e74c3c", "#9b59b6", "#3498db", "#95a5a6", "#34495e"]
sns.set_palette(flatui)

fig, ax = plt.subplots()

ax2 = ax.twinx()

sns.violinplot(x='name',
               y='plotValue',
               data=df_full,
               width=0.9,
               hue='name_value',
               split=True,
               cut = 0,
               #palette='Greens',
               ax=ax
              )

ax.set(ylabel = "$\mathregular{log_{10}}$ (Commits)", xlabel = None)
ax2.set(ylabel = "$\mathregular{log_{10}}$ (Age [days])", xlabel = None)
ax.set_xticklabels(ax.get_xticklabels(), rotation=45)

ax.legend(frameon=True, loc = "upper right") 

ax.set_ylim([-0.1, 5.3])
ax2.set_ylim([-0.1, 5.3])

plt.tight_layout()
plt.savefig(f"{r.GRAPHS_DIR}/fig7_project_size_and_age_distribution.pdf")
plt.show()
```
