~~~
NOTE

This is an R markdown document. It's like a markdown document, but has 
executable snippets of code that do the work (in various languages, including
bash, Python, R). It can be viewed using RStudio---we recommend doing this.

    rstudio 2.3-case-study-data-analysis-and-graphs

It can also be executed (in whole) with R using the following bash snippet:*
 
    Rscript -e "rmarkdown::render('2.3-case-study-data-analysis-and-graphs.Rmd')"

This will compile the document into HTML while executing all the code 
snippets in the document.
~~~


# Part 2.3. Case study: data analysis and graphs

> Note that the graphs created in this section are very sensitive to the actual
> dataset and in cases to proper sampling used. By default, both have been
> substantially limited for the purpose of the artifact and therefore the
> obtained distributions and models might look quite different from the graphs
> published in the paper. This is not an error.

This section prepares plots the graphs for Section 4 *A Case Study: Of Bugs and
Languages* in the paper. The document contains a section for each of the four
plots in the paper and a section for preparing the data.

- Preparation
- Random subsets (Fig. 4)
- TypeScript over time (Fig. 5)
- Domain knowledge (Fig. 6)
- Domain knowledge (Fig. 7)

The graphs can be generated in any order, but all of them depend on the
preparation section having been executed. Scripts in this section do not
download anything and run in reasonable times. 

All the graphs are generated in place in the RMD file as well as into PDFs in
`/graphs`.

## Preparation 

Load auxiliary functions and initialize log:

```{R}
use_python("/usr/local/bin/python")
knitr::knit_engines$set(python = reticulate::eng_python)
source("scripts/implementation.R")
initializeLog()
```

Load the results of the queries:

```{r}
INPUT_DIR <- "query-results/paper/" # output of queries, input of this script
MODEL_DIR <- "models/"
GRAPHS_DIR <- "graphs/"

dir.create(MODEL_DIR)
dir.create(GRAPHS_DIR)

STARS <- paste0(INPUT_DIR, "stars.csv")
EXPERIENCED_AUTHORS <- paste0(INPUT_DIR, "experienced_authors.csv")
EXPERIENCED_AUTHORS_RATIO <- paste0(INPUT_DIR, "experienced_authors_ratio.csv")
MEAN_CHANGES_IN_COMMITS <- paste0(INPUT_DIR, "mean_changes_in_commits.csv")
MEDIAN_COMMIT_MESSAGE_SIZES <- paste0(INPUT_DIR, "median_commit_message_sizes.csv")
COMMITS <- paste0(INPUT_DIR, "commits.csv")
ALL_ISSUES <- paste0(INPUT_DIR, "all_issues.csv")
FULL_DATASET <- paste0(INPUT_DIR, "full_dataset.csv")

FSE_REFERENCE <- "query-results/fse/reference.csv"

datasets_raw <- list(
  stars = load_dataset(STARS),
  changes = load_dataset(MEAN_CHANGES_IN_COMMITS),
  expa = load_dataset(EXPERIENCED_AUTHORS),
  expr = load_dataset(EXPERIENCED_AUTHORS_RATIO),
  messages = load_dataset(MEDIAN_COMMIT_MESSAGE_SIZES),
  commits = load_dataset(COMMITS),
  issues = load_dataset(ALL_ISSUES)
)

all_languages = read_delim(FSE_REFERENCE, delim=',', escape_double=FALSE, escape_backslash=TRUE, quote="\"")$X1
all_languages = all_languages[-(1:5)]
```

Now we fix the datasets, calculating the missing commit age and filtering small
projects and small project-language pairs:

```{r}
fix_dataset = function(dataset, dataset_name) {
    before = nrow(dataset)
    # remove projects which have fewer than 28 commits 
    small_projects = dataset %>% group_by(project) %>% summarize(commits = n_distinct(sha)) %>% filter(commits >= 28)
    dataset = dataset %>% filter(project %in% small_projects$project)
    # calculate commit age
    dataset = dataset %>% group_by(project) %>% mutate(min_date = min(commit_date)) %>% mutate(commit_age = max(1, as.integer((commit_date - min_date)/(24 * 3600)))) %>% dplyr::select(-c(min_date))
    # and remove small groups, i.e. those that contain less than 20 commits (this was done in the original paper
    dataset = dataset %>% group_by(project, language) %>% mutate(n = n()) %>% filter(n > 20) %>% dplyr::select(-c(n))
    # we are done    
    after = nrow(dataset)
    LOG(paste0("selection",dataset_name,"before"), before)
    LOGPctAndRaw(paste0("selection",dataset_name,"after"), after, before)
    # and return the updated dataset
    dataset
}
datasets = Map(fix_dataset, datasets_raw, names(datasets_raw))
```

Calculate the coefficients for various languages:

```{r}
summarizeByLanguage = function(what) {
    what %>% 
        group_by(project, language) %>%
        dplyr::summarize(
            commits = n_distinct(sha),
            tins = sum(insertion),
            max_commit_age = max(commit_age),
            bcommits = sum(isbug),
            #domain = unique(domain),
            devs = n_distinct(devs)
        )
}

logTransform = function(what, log1 = log, log2 = log) {
    data.frame(
        language = what$language, 
        ldevs = log1(what$devs),
        lcommits=log1(what$commits),
        lmax_commit_age=log1(what$max_commit_age),
        lbcommits=log2(what$bcommits + 0.5*(what$bcommits==0)),
        bcommits=what$bcommits,
        #combined=factor(what$combined),
        #domain=factor(what$domain),
        #domain_r = relevel(what$domain, rev(levels(what$domain))[1]),
        language_r = relevel(what$language, rev(levels(what$language))[1]),
        commits = what$commits
        #combined_r = relevel(what$combined, rev(levels(what$combined))[1])
    )
}

# Weighted contrasts as described and used by the authors of the original paper
contr.Weights <- function(fac)
{
    fDist=summary(fac)
    fSum=contr.sum(levels(fac))		
    fSum[nrow(fSum),] = -fDist[1:ncol(fSum)]/fDist[length(fDist)]
    fSum
}

# Takes the glm model and the releveled second model for the last observation and combines them together returning a single data frame
combineModels = function(model, model_r, var, pValAdjust = "none") {
    controlVariables = 3
    s = summary(model)$coefficients
    s_r = summary(model_r)$coefficients
    rownames = getModelRowNames(model, var)
    coef = round(c(s[,1], s_r[controlVariables + 2, 1]), 2)
    se = round(c(s[,2], s_r[controlVariables + 2, 2]), 2)
    pVal = c(s[,4], s_r[controlVariables + 2, 4])
    if (pValAdjust == "bonferroni" || pValAdjust == "fdr")
        pVal[(controlVariables + 2):length(pVal)] = p.adjust(pVal[(controlVariables + 2):length(pVal)], pValAdjust)
    #pVal = round(pVal, 3)
    names(coef) = rownames
    data.frame(
        coef, 
        se,
        pVal
    )
} 

getModelRowNames = function(model, var) {
    controlVariables = 3
    rownames = c(dimnames(summary(model)$coefficients)[[1]][1:(1 + controlVariables)], names(summary(var)))
    names(rownames) = rownames
    rownames[["(Intercept)"]] = "Intercept"
    rownames[["lmax_commit_age"]] = "log age"
    rownames[["ldevs"]] = "log devs"
    rownames[["lcommits"]] = "log commits"
    rownames
}

calculateModel = function(dataset, dataset_name) {
    cat(paste0(nrow(dataset)," ", dataset_name, "\n"))
    X = summarizeByLanguage(dataset)
    X$language = droplevels(X$language)
    #Y = logTransform(X, log10, log)
    Y = logTransform(X, log, log)
    # fit the negative binomial regression
    weights = contr.Weights(Y$language)
    nbfit = glm.nb(bcommits~lmax_commit_age+ldevs+lcommits+language, contrasts = list(language = contr.Weights(Y$language)), data=Y)
    nbfit_r = glm.nb(bcommits~lmax_commit_age+ldevs+lcommits+language_r, contrasts = list(language_r = contr.Weights(Y$language_r)), data=Y)
    # combine them into single result table
    result = combineModels(nbfit, nbfit_r, Y$language)
    result$pVal = round(result$pVal, digits = 3)
    result$name = as.factor(rownames(result))
    result$dataset = dataset_name
    result$signifficant = result$pVal <= 0.05 # orly? 
    full_names = data.frame(name = as.factor(c("Intercept", "log age", "log devs", "log commits", all_languages)))
    result = left_join(full_names, result, by = c("name"))
    rownames(result) = full_names$name
    result
}

models = Map(calculateModel, datasets, names(datasets))
Map(function(model, name) {write.csv(model %>% dplyr::select(coef, pVal), paste0(MODEL_DIR,"/",name,".csv"))} , models, names(models))
```

#### Random subsets

Our first experiment explores the distribution of possible analysis outcomes.
For this, we repeatedly pick a random subset of 50 projects of each of the 17
languages and fit them with NBR. Fig 4 shows the distribution of the
coefficients obtained by 1000 such random selections compared to the results
obtained in [FSE2014] (shown as a tick to the right of the distribution).
Positive values indicate a higher association of the language with defects.
The spread of each distribution is a measure of the sensitivity of the analysis
to its inputs.  Intuitively, consider the distribution of coefficients for
Objective-C, it is roughly centered around 0. This means, that a random input
is about equally likely to say that the language has a positive association
with defects as a negative one. One could argue that picking close to the
median of the distribution could give a representative answer. As we can see
the FSE paper often picks subsets that are outliers; see the cases of
CoffeeScript, Go, Perl, Scala and most strikingly TypeScript.

Please note that the number of samples used for generating the graph should not
be larger than the number of samples generated during post-processing the
dataset (10000 by default). Also note that the more samples are used, the
slower the processing becomes.

```{python, fig.width=14, fig.height=8}
SAMPLES = 100

import numpy as np
import pandas as pd
import matplotlib as mpl
mpl.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.colors import LogNorm
from os import listdir
from os.path import isfile, join

sns.set(style='white', rc={'figure.figsize':(14,8)}) #24,8 for big one
plt.rcParams.update({'font.size': 42})

MICRO_SIZE = 16
SMALL_SIZE = 20
MEDIUM_SIZE = 26
BIGGER_SIZE = 32

plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title
plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels
plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('legend', fontsize=MICRO_SIZE)    # legend fontsize
plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title

ref_FSE = [0.15, 0.23, 0.03, -0.29, -0.07, 0, -0.08, -0.23, -0.01, 0.06, 0.18, -0.15, 0.15, 0.1, -0.15, -0.28, -0.43]

frames = []

for i in range(1, SAMPLES + 1):
    temp = pd.read_csv(f'{r.MODEL_DIR}/ts_by_year/2020/{str(i)}2020_new.csv')
    frames.append(temp[4:])

ref = pd.read_csv(r.FSE_REFERENCE)[5:]
ref['coef'] = ref_FSE

df_dist = pd.concat(frames, ignore_index=True, sort=False)
df_dist = pd.concat([df_dist, ref], ignore_index=True, sort=False)
df_dist['split'] = ['ours']*(len(df_dist) - 17) + ['their']*17
df_dist.rename(columns={'Unnamed: 0': 'Language:'}, inplace=True)

plt.figure(figsize=(14,8))
f = sns.violinplot(x='Language:',
               y='coef',
               data=df_dist,
               width=0.9,
               palette='Greens',
               hue='split',
               split=True
              )
plt.axhline(0, ls='--', color='black')
plt.legend().set_visible(False)
plt.title('')
plt.xlabel(None)
plt.ylabel('Coefficient')
f.set_xticklabels(f.get_xticklabels(), rotation=90)
plt.ylim((-0.7, 0.65))

plt.axhline(0.54, 0.77, 0.82, ls='-', color='black')
plt.text(13.8, 0.52, 'FSE 2014')

plt.tight_layout()
plt.savefig(f"{r.GRAPHS_DIR}/fig4_random_subsets.pdf")
plt.show()
```

#### TypeScript over time

As we have more data than was available in 2013, we can use Code DJ to select
inputs at various times. Here we create eight datasets, each containing data up
to one of the years between 2013 and 2020. For simplicity, we only plot the
distribution of coefficients for TypeScript. The original paper’s coefficient
was −.43 (shown as a red line). The graph clearly shows that the value was an
outlier. The association with bugs shifted over time, increasing to a
relatively stable position from 2016.

While it is reasonable to expect variations from year to year, TypeScript
experienced a rather large shift over a short period. The language was released
in 2012, so there were few projects on GitHub in 2013. Furthermore, a number of
human language translation files were misidentified as TypeScript; these files
did not have bugs, biasing the result. The rising popularity of TypeScript
quickly caused real code to crowd out the translation files, and the
association with bugs settled to around 0.2.

Please note that the number of samples used for generating the graph should not
be larger than the number of samples generated during post-processing the
dataset (100 for the artifact, but 10000 was used for the paper). Also note
that the more samples are used, the slower the processing becomes.

```{python, fig.width=14, fig.height=8}
SAMPLES = 100

import numpy as np
import pandas as pd
import matplotlib as mpl
mpl.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.colors import LogNorm
from os import listdir
from os.path import isfile, join

sns.set(style='white', rc={'figure.figsize':(14,8)}) #24,8 for big one
plt.rcParams.update({'font.size': 42})

MICRO_SIZE = 16
SMALL_SIZE = 20
MEDIUM_SIZE = 26
BIGGER_SIZE = 32

plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title
plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels
plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('legend', fontsize=MICRO_SIZE)    # legend fontsize
plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title

years = range(2013, 2021)
frames_concat = []
for year in years:
    frames = []
    print(year)
    temp_concat = []
    for i in range(1, SAMPLES + 1):
        #file =   f'{r.MODEL_DIR}/by_year/{str(year)}/{str(i)}{str(year)}_new.csv'
        file = f'{r.MODEL_DIR}/ts_by_year/{str(year)}/{str(i)}{str(year)}_new.csv'
        print(file)
        temp = pd.read_csv(file)
        temp['year'] = [int(year)]*len(temp)
        frames.append(temp[21:])
        temp_concat = pd.concat(frames, ignore_index=True, sort=False)
        
    frames_concat.append(temp_concat)
    
df_concat = pd.concat(frames_concat, ignore_index=True, sort=False)
df_concat.rename(columns={'Unnamed: 0': 'Language:'}, inplace=True)

df_ref = pd.DataFrame({'year': [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020],
                       'coef': [-0.43]*8,
                       'Language:': ['Typescript']*8,
                       'check' : ['issue']*8
                      })
                      
df_concat = pd.concat([df_concat, df_ref], ignore_index=True, sort=False)                  
df_concat['check'] = ['all']*(len(df_concat) - 1) + ['issues']

#colors = ["faded green", "pale red", "windows blue", "amber", "greyish", "dusty purple"]
#sns.palplot(sns.xkcd_palette(colors))

plt.figure(figsize=(24,12))
f = sns.violinplot(x='year',
               y='coef',
               data=df_concat,
               width=0.9,
               split=True,
               hue='check',
               palette='Greens'
              )
plt.axhline(0, ls='--', color = 'black')
plt.legend().set_visible(False)
#plt.title('.TS over time')
plt.xlabel(None)
plt.ylabel('Coefficient')
f.set_xticklabels(f.get_xticklabels(), rotation=45)
plt.text(2.4, -0.6, 'FSE 2014')
plt.ylim((-0.72, 0.6))
plt.axhline(-0.43, 0.0, 1.0, ls='-', color='red')

#plt.tight_layout()
plt.savefig(f"{r.GRAPHS_DIR}/fig5_typescript_over_time.pdf")
plt.show()
```

#### Domain knowledge

Creates a figure that, for each language, calculates the value of the
coefficients (higher means more bugs); the queries returned 50 projects in each
of the 17 target languages: Coefficients that are not statistically significant
are shown in faded colors. If the input set did not matter for the model, one
could expect the different queries to give roughly the same coefficients with
the same significance. This is not the case. If we focus on how many languages
have statistically significant coefficients: The touched files query is highly
predictive, 14 of the languages are significant, but the coefficients are
frequently opposite from those of other queries. Specifically, C is associated
with slightly fewer bugs, so are C#, CoffeeScript, Java, JavaScript,
Objective-C, Perl, PHP, Python, Ruby and TypeScript. On the other hand C++,
Erlang, Go and Haskell are associated with more defects. This is striking as it
goes against expectations. The stars query is the least informative. It only
gives 7 statistically significant coefficients with remarkably low values.

```{r}
summarizeByLanguage = function(what) {
    what %>% 
        group_by(project, language) %>%
        dplyr::summarize(
            commits = n_distinct(sha),
            tins = sum(insertion),
            max_commit_age = max(commit_age),
            bcommits = sum(isbug),
            #domain = unique(domain),
            devs = n_distinct(devs)
        )
}

logTransform = function(what, log1 = log, log2 = log) {
    data.frame(
        language = what$language, 
        ldevs = log1(what$devs),
        lcommits=log1(what$commits),
        lmax_commit_age=log1(what$max_commit_age),
        lbcommits=log2(what$bcommits + 0.5*(what$bcommits==0)),
        bcommits=what$bcommits,
        #combined=factor(what$combined),
        #domain=factor(what$domain),
        #domain_r = relevel(what$domain, rev(levels(what$domain))[1]),
        language_r = relevel(what$language, rev(levels(what$language))[1]),
        commits = what$commits
        #combined_r = relevel(what$combined, rev(levels(what$combined))[1])
    )
}

# Weighted contrasts as described and used by the authors of the original paper
contr.Weights <- function(fac)
{
    fDist=summary(fac)
    fSum=contr.sum(levels(fac))		
    fSum[nrow(fSum),] = -fDist[1:ncol(fSum)]/fDist[length(fDist)]
    fSum
}

# Takes the glm model and the releveled second model for the last observation and combines them together returning a single data frame
combineModels = function(model, model_r, var, pValAdjust = "none") {
    controlVariables = 3
    s = summary(model)$coefficients
    s_r = summary(model_r)$coefficients
    rownames = getModelRowNames(model, var)
    coef = round(c(s[,1], s_r[controlVariables + 2, 1]), 2)
    se = round(c(s[,2], s_r[controlVariables + 2, 2]), 2)
    pVal = c(s[,4], s_r[controlVariables + 2, 4])
    if (pValAdjust == "bonferroni" || pValAdjust == "fdr")
        pVal[(controlVariables + 2):length(pVal)] = p.adjust(pVal[(controlVariables + 2):length(pVal)], pValAdjust)
    #pVal = round(pVal, 3)
    names(coef) = rownames
    data.frame(
        coef, 
        se,
        pVal
    )
} 

getModelRowNames = function(model, var) {
    controlVariables = 3
    rownames = c(dimnames(summary(model)$coefficients)[[1]][1:(1 + controlVariables)], names(summary(var)))
    names(rownames) = rownames
    rownames[["(Intercept)"]] = "Intercept"
    rownames[["lmax_commit_age"]] = "log age"
    rownames[["ldevs"]] = "log devs"
    rownames[["lcommits"]] = "log commits"
    rownames
}

calculateModel = function(dataset, dataset_name) {
    cat(paste0(nrow(dataset)," ", dataset_name, "\n"))
    X = summarizeByLanguage(dataset)
    X$language = droplevels(X$language)
    #Y = logTransform(X, log10, log)
    Y = logTransform(X, log, log)
    # fit the negative binomial regression
    weights = contr.Weights(Y$language)
    nbfit = glm.nb(bcommits~lmax_commit_age+ldevs+lcommits+language, contrasts = list(language = contr.Weights(Y$language)), data=Y)
    nbfit_r = glm.nb(bcommits~lmax_commit_age+ldevs+lcommits+language_r, contrasts = list(language_r = contr.Weights(Y$language_r)), data=Y)
    # combine them into single result table
    result = combineModels(nbfit, nbfit_r, Y$language)
    result$pVal = round(result$pVal, digits = 3)
    result$name = as.factor(rownames(result))
    result$dataset = dataset_name
    result$signifficant = result$pVal <= 0.05 # orly? 
    full_names = data.frame(name = as.factor(c("Intercept", "log age", "log devs", "log commits", all_languages)))
    result = left_join(full_names, result, by = c("name"))
    rownames(result) = full_names$name
    result
}

output_model = function(model, name) {
  #model$coef[model$coef > 0.5] = 0.5
  #model$coef[model$coef < -0.5] = -0.5
  write.csv(model %>% dplyr::select(coef, pVal), paste0(MODEL_DIR, name, ".csv"))
}

models <- Map(calculateModel, datasets, names(datasets))
Map(output_model, models, names(models))
```

Use these models to create a graph:


```{python fig.width=24, fig.height:8}
import numpy as np
import pandas as pd
import matplotlib as mpl
mpl.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
import time
from datetime import datetime
from matplotlib.colors import LogNorm
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.patches as mpatches

sns.set(style='white', rc={'figure.figsize':(14,8)})
plt.rcParams.update({'font.size': 42})

MICRO_SIZE = 16
SMALL_SIZE = 20
MEDIUM_SIZE = 26
BIGGER_SIZE = 32

plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title
plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels
plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('legend', fontsize=MICRO_SIZE)    # legend fontsize
plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title

from os import listdir
from os.path import isfile, join

files = ['stars.csv', 'expr.csv', 'expa.csv', 'commits.csv', 'messages.csv', 'issues.csv', 'changes.csv']

names = ['Stars',
      '50% Experienced',
      'Experienced Author',
      'Number of Commits',
      'Message Size',
      'Issues',
      'Touched Files'
      ]

frames = []

for f in files:
    temp = pd.read_csv(r.MODEL_DIR + f)[4:]
    print("temp")
    print(temp)
    frames.append(temp)

df_smart = pd.concat(frames, ignore_index=True, sort=False)
df_smart.rename(columns={'Unnamed: 0': 'Language:'}, inplace=True)
df_smart['split'] = sum([[f]*17 for f in files], []) #change to names

df_smart_sig = df_smart.copy()
df_smart_insig = df_smart.copy()

df_smart_sig.loc[(df_smart_sig['pVal'] <= 0.05), 'coef'] = 0.0
df_smart_insig.loc[(df_smart_insig['pVal'] > 0.05), 'coef'] = 0.0

fig, ax = plt.subplots()
ax2 = ax
ax3 = ax

sns.barplot(x='Language:',
               y='coef',
               data=df_smart_sig,
               hue='split',
               #palette='Greens',
               edgecolor='k',
               #color=('blue'),
               ax = ax)

sns.barplot(x='Language:',
               y='coef',
               data=df_smart_insig,
               hue='split',
               #palette='Greens',
               edgecolor='k',
               alpha = 1.0,
               ax=ax2,
               #legend=False,
               color='white')

      
sns.barplot(x='Language:',
               y='coef',
               data=df_smart_insig,
               hue='split',
               #palette='Greens',
               edgecolor='k',
               alpha = 0.2,
               ax=ax3,
               #legend=False
               #color=('blue'),
               )         

plt.xlabel(None)
plt.ylabel('Coefficient')
ax.set_xticklabels(ax.get_xticklabels(), rotation=45)
plt.xlim((-0.42, 16.43))


handles, labels = ax.get_legend_handles_labels()
ax.legend(handles[0:7], names[0:7], frameon=False, loc = "best") 

print(handles)
print(labels)

for i in range(9):
    ax.axvspan(-0.4 + 2*i, 0.60 + 2*i, facecolor='gray', alpha=0.1)

offset = 0.0632

plt.tight_layout()
plt.savefig(f"{r.GRAPHS_DIR}/fig6_domain_knowledge.pdf") # Domain knowledge
plt.show()
```

The plot is created in `graphs/fig6_domain_knowledge.pdf`.

### Project Size and Age Distributions

To help, the paper provides distributions of various measures in the data. This
figure visualizes the distribution of project sizes (left) and project age
(right) for the entire dataset and for the all the queries queries.

Looking at these distributions makes it clear that the projects returned by
various queries are quite different in character. The experienced author and
the number of commits are remarkably similar and return projects that meet our
expectations. The issues distribution is similar, which should raise red flags
given that it frequently disagrees. The stars query returns many smaller
projects. Finally, message sizes and touched files show distributions opposite
to those expected. They favor degenerate young projects with few commits that
are either verbose, or disproportionately large (touching over 100k files).
This is reflected also in the input sizes, ranging from 8M rows for the
experienced author query to mere 79K rows of the touched files query. It is
likely that these queries are “wrong” in the sense they do not return the
population of interest. The Figure also suggest that stars is not a good
choice.

```{python fig.width=14, fig.height:9}
import numpy as np
import pandas as pd
import matplotlib as mpl
mpl.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.colors import LogNorm
from os import listdir
from os.path import isfile, join

stuff = [f for f in listdir(r.INPUT_DIR) if isfile(join(r.INPUT_DIR, f))]

df_names = ['full_dataset_p.csv',
            'stars_p.csv',
            'experienced_authors_ratio_p.csv',
            'experienced_authors_p.csv',
            'commits_p.csv',
            'median_commit_message_sizes_p.csv',
            'all_issues_p.csv',
            'mean_changes_in_commits_p.csv',
          ]

names = ['Full dataset',
         'Stars',
         '50% Experienced',     
         'Experienced Author',
         'Number of Commits',
         'Message Size',      
         'Issues',
         'Touched Files'
        ]

frames = []

for i,f in enumerate(df_names):
    temp = pd.read_csv(r.INPUT_DIR + f)
    temp['name'] = [names[i]] * len(temp.index)
    frames.append(temp)

df_full = pd.concat(frames, ignore_index=True, sort=False)
df_full['plotValue'] = np.log10(df_full.commits)
df_full['name_value'] = ['Commits']*len(df_full.index)

df_full_age = pd.concat(frames, ignore_index=True, sort=False)
df_full_age['plotValue'] = np.log10(df_full.age/3600/24+1)
df_full_age['name_value'] = ['Age [days]']*len(df_full.index)

df_full = pd.concat([df_full, df_full_age], ignore_index=True, sort=False)

sns.set(style='white', rc={'figure.figsize':(14,8)}) #24,8 for big one
plt.rcParams.update({'font.size': 42})

MICRO_SIZE = 16
SMALL_SIZE = 20
MEDIUM_SIZE = 26
BIGGER_SIZE = 32

plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title
plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels
plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('legend', fontsize=MICRO_SIZE)    # legend fontsize
plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title

flatui = ["#82E0AA", "#FF5733", "#2ecc71", "#e74c3c", "#9b59b6", "#3498db", "#95a5a6", "#34495e"]
sns.set_palette(flatui)

fig, ax = plt.subplots()

ax2 = ax.twinx()

sns.violinplot(x='name',
               y='plotValue',
               data=df_full,
               width=0.9,
               hue='name_value',
               split=True,
               cut = 0,
               #palette='Greens',
               ax=ax
              )

ax.set(ylabel = "$\mathregular{log_{10}}$ (Commits)", xlabel = None)
ax2.set(ylabel = "$\mathregular{log_{10}}$ (Age [days])", xlabel = None)
ax.set_xticklabels(ax.get_xticklabels(), rotation=45)

ax.legend(frameon=True, loc = "upper right") 

ax.set_ylim([-0.1, 5.3])
ax2.set_ylim([-0.1, 5.3])

plt.tight_layout()
plt.savefig(f"{r.GRAPHS_DIR}/fig7_project_size_and_age_distribution.pdf")
plt.show()
```
